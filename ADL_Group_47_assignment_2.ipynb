{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ded42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Initial files : 202599\n",
      "Kept files    : 20000\n",
      "Deleted files : 182599\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# ===== CONFIG =====\n",
    "DATA_DIR = \"data/img_align_celeba\"\n",
    "KEEP_COUNT = 20000\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
    "\n",
    "# ==================\n",
    "\n",
    "# List all image files\n",
    "files = [\n",
    "    f for f in os.listdir(DATA_DIR)\n",
    "    if f.lower().endswith(IMAGE_EXTS)\n",
    "]\n",
    "\n",
    "total_files = len(files)\n",
    "\n",
    "if total_files <= KEEP_COUNT:\n",
    "    print(f\"Only {total_files} files found. Nothing to delete.\")\n",
    "    exit()\n",
    "\n",
    "# Randomly select files to keep\n",
    "keep_files = set(random.sample(files, KEEP_COUNT))\n",
    "\n",
    "deleted = 0\n",
    "for f in files:\n",
    "    if f not in keep_files:\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        deleted += 1\n",
    "\n",
    "print(\"===================================\")\n",
    "print(f\"Initial files : {total_files}\")\n",
    "print(f\"Kept files    : {KEEP_COUNT}\")\n",
    "print(f\"Deleted files : {deleted}\")\n",
    "print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc7aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18817e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/img_align_celeba\"\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=os.path.dirname(DATA_DIR), transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76aa2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),   # 64 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # 32 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),# 16 → 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),# 8 → 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, LATENT_DIM)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_dec = nn.Linear(LATENT_DIM, 256 * 4 * 4)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(enc), self.fc_logvar(enc)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        dec = self.fc_dec(z).view(-1, 256, 4, 4)\n",
    "        return self.decoder(dec), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon, x, reduction='sum')\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b18a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Loss: 2038.8911\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m recon, mu, logvar \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(recon, imgs, mu, logvar)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = model(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f43878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs.to(device)\n",
    "    recon, _, _ = model(imgs)\n",
    "\n",
    "comparison = torch.cat([imgs[:8], recon[:8]])\n",
    "grid = make_grid(comparison.cpu(), nrow=8, normalize=True)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top: Original | Bottom: Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9201f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(z1, z2, steps=10):\n",
    "    return [(1 - t) * z1 + t * z2 for t in torch.linspace(0, 1, steps)]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs[:2].to(device)\n",
    "\n",
    "    _, mu, _ = model(imgs)\n",
    "    z_interp = interpolate(mu[0], mu[1])\n",
    "\n",
    "    decoded = []\n",
    "    for z in z_interp:\n",
    "        out = model.decoder(model.fc_dec(z.unsqueeze(0)).view(1,256,4,4))\n",
    "        decoded.append(out)\n",
    "\n",
    "grid = make_grid(torch.cat(decoded), nrow=len(decoded), normalize=True)\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.imshow(np.transpose(grid.cpu(), (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Latent Space Interpolation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871690c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector(model, dataloader, n=100):\n",
    "    zs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            _, mu, _ = model(imgs)\n",
    "            zs.append(mu)\n",
    "            if i * imgs.size(0) > n:\n",
    "                break\n",
    "    return torch.cat(zs).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f356c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vectors (placeholders for attributes)\n",
    "smile_vector = get_latent_vector(model, dataloader)\n",
    "neutral_vector = get_latent_vector(model, dataloader)\n",
    "\n",
    "attribute_vector = smile_vector - neutral_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e26e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply attribute\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img, _ = next(iter(dataloader))\n",
    "    img = img[:1].to(device)\n",
    "\n",
    "    _, mu, _ = model(img)\n",
    "    modified_z = mu + 0.8 * attribute_vector\n",
    "\n",
    "    out = model.decoder(model.fc_dec(modified_z).view(1,256,4,4))\n",
    "\n",
    "comparison = torch.cat([img, out])\n",
    "grid = make_grid(comparison.cpu(), nrow=2, normalize=True)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original vs Attribute Modified\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
