{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ded42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Initial files : 202599\n",
      "Kept files    : 20000\n",
      "Deleted files : 182599\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# ===== CONFIG =====\n",
    "DATA_DIR = \"data/img_align_celeba\"\n",
    "KEEP_COUNT = 20000\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
    "\n",
    "# ==================\n",
    "\n",
    "# List all image files\n",
    "files = [\n",
    "    f for f in os.listdir(DATA_DIR)\n",
    "    if f.lower().endswith(IMAGE_EXTS)\n",
    "]\n",
    "\n",
    "total_files = len(files)\n",
    "\n",
    "if total_files <= KEEP_COUNT:\n",
    "    print(f\"Only {total_files} files found. Nothing to delete.\")\n",
    "    exit()\n",
    "\n",
    "# Randomly select files to keep\n",
    "keep_files = set(random.sample(files, KEEP_COUNT))\n",
    "\n",
    "deleted = 0\n",
    "for f in files:\n",
    "    if f not in keep_files:\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        deleted += 1\n",
    "\n",
    "print(\"===================================\")\n",
    "print(f\"Initial files : {total_files}\")\n",
    "print(f\"Kept files    : {KEEP_COUNT}\")\n",
    "print(f\"Deleted files : {deleted}\")\n",
    "print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc7aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18817e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/img_align_celeba\"\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=os.path.dirname(DATA_DIR), transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76aa2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),   # 64 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # 32 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),# 16 → 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),# 8 → 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, LATENT_DIM)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_dec = nn.Linear(LATENT_DIM, 256 * 4 * 4)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(enc), self.fc_logvar(enc)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        dec = self.fc_dec(z).view(-1, 256, 4, 4)\n",
    "        return self.decoder(dec), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon, x, reduction='sum')\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = model(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f43878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs.to(device)\n",
    "    recon, _, _ = model(imgs)\n",
    "\n",
    "comparison = torch.cat([imgs[:8], recon[:8]])\n",
    "grid = make_grid(comparison.cpu(), nrow=8, normalize=True)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top: Original | Bottom: Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9201f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(z1, z2, steps=10):\n",
    "    return [(1 - t) * z1 + t * z2 for t in torch.linspace(0, 1, steps)]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs[:2].to(device)\n",
    "\n",
    "    _, mu, _ = model(imgs)\n",
    "    z_interp = interpolate(mu[0], mu[1])\n",
    "\n",
    "    decoded = []\n",
    "    for z in z_interp:\n",
    "        out = model.decoder(model.fc_dec(z.unsqueeze(0)).view(1,256,4,4))\n",
    "        decoded.append(out)\n",
    "\n",
    "grid = make_grid(torch.cat(decoded), nrow=len(decoded), normalize=True)\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.imshow(np.transpose(grid.cpu(), (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Latent Space Interpolation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871690c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector(model, dataloader, n=100):\n",
    "    zs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            _, mu, _ = model(imgs)\n",
    "            zs.append(mu)\n",
    "            if i * imgs.size(0) > n:\n",
    "                break\n",
    "    return torch.cat(zs).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f356c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vectors (placeholders for attributes)\n",
    "smile_vector = get_latent_vector(model, dataloader)\n",
    "neutral_vector = get_latent_vector(model, dataloader)\n",
    "\n",
    "attribute_vector = smile_vector - neutral_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e26e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply attribute\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img, _ = next(iter(dataloader))\n",
    "    img = img[:1].to(device)\n",
    "\n",
    "    _, mu, _ = model(img)\n",
    "    modified_z = mu + 0.8 * attribute_vector\n",
    "\n",
    "    out = model.decoder(model.fc_dec(modified_z).view(1,256,4,4))\n",
    "\n",
    "comparison = torch.cat([img, out])\n",
    "grid = make_grid(comparison.cpu(), nrow=2, normalize=True)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original vs Attribute Modified\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
